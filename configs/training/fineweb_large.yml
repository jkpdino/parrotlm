# FineWeb-Edu GPT Training Configuration
# Medium/large model for training on FineWeb-Edu (~4B tokens config)

name: fineweb_gpt_large
random_seed: 42

# Model architecture
model:
  vocab_size: 32000 # LLaMA 2 vocab size
  dim: 768 # Model dimension
  depth: 16 # Number of transformer layers
  n_heads: 16 # Number of query heads
  n_kv_heads: 8 # Number of key/value heads (GQA)
  context_length: 512 # Maximum sequence length
  dropout: 0.0 # Dropout (modern LLMs use 0.0)
  ffn_hidden_mult: 4 # FFN hidden dimension multiplier
  pad_token_id: 0 # Padding token ID
  rope_scaling_factor: 1.0 # RoPE cache scaling factor
  use_gradient_checkpointing: false # Disable for speed; enable if memory bound

# Data configuration
data:
  dataset_path: "datasets/fineweb_edu_4b/pretokenized/tokenizer" # Update if you use a different output path
  tokenizer_path: "meta-llama/Llama-2-7b-hf" # Use LLaMA 2 7B tokenizer from HF Hub
  batch_size: 64 # Adjust based on GPU memory; use accumulate_steps if needed
  shuffle: false
  num_workers: 0

# Training hyperparameters
training:
  learning_rate: 0.003
  max_batches: null
  max_tokens: 1000000000
  optimizer: "adamw"
  warmup_batches: 100
  weight_decay: 0.1
  grad_clip: 1.0
  lr_schedule: "cosine"
  min_lr_ratio: 0.1
  accumulate_steps: 4 # Increase effective batch without extra memory

# Checkpointing
checkpointing:
  save_every_n_batches: 100
  keep_last_n: 10
  output_dir: "training_runs"

# Monitoring and validation
monitoring:
  validate_every_n_batches: 1000
  validation_samples: 50
  inference_every_n_batches: 500
  inference_prompt: "Once upon a time"
  inference_max_tokens: 50
  inference_temperature: 1.0
  graph_window_size: 1000
