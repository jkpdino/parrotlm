# TinyStories GPT Training Configuration
# Small model (~1.8M parameters) for training on TinyStories dataset

name: tinystories_gpt_medium
random_seed: 42

# Model architecture
model:
  vocab_size: 32768 # Match your tokenizer vocab size
  dim: 768 # Model dimension
  depth: 16 # Number of transformer layers
  n_heads: 16 # Number of query heads
  n_kv_heads: 8 # Number of key/value heads (GQA: 3:1 ratio)
  context_length: 512 # Maximum sequence length
  dropout: 0.0 # Dropout (modern LLMs use 0.0)
  ffn_hidden_mult: 4 # FFN hidden dimension multiplier
  pad_token_id: 0 # Padding token ID
  rope_scaling_factor: 1.0 # RoPE cache scaling factor
  use_gradient_checkpointing: false # Disabled for speed (you have 24GB VRAM!)

# Data configuration
data:
  dataset_path: "datasets/tinystories/pretokenized/tokenizer"
  tokenizer_path: "tokenizers/tinystories_tokenizer/tokenizer.json"
  batch_size: 64 # Batch size (adjust based on GPU memory)
  shuffle: false # Shuffle training data
  num_workers: 0 # DataLoader workers (0 for single-threaded)

# Training hyperparameters
training:
  learning_rate: 0.003 # Learning rate (Muon works well with higher LR)
  max_batches: null # Maximum number of batches
  max_tokens: 1000000000 # Or specify max tokens instead
  optimizer: "adamw" # "muon" or "adamw"
  warmup_batches: 100 # Linear warmup batches
  weight_decay: 0.1 # Weight decay
  grad_clip: 1.0 # Gradient clipping (null to disable)
  lr_schedule: "cosine" # "constant", "cosine", or "linear"
  min_lr_ratio: 0.1 # Minimum LR as ratio of base LR
  accumulate_steps: 4 # Gradient accumulation to increase effective batch without extra memory

# Checkpointing
checkpointing:
  save_every_n_batches: 100 # Save checkpoint every N batches
  keep_last_n: 10 # Keep last N numbered checkpoints
  output_dir: "training_runs" # Base directory for training runs

# Monitoring and validation
monitoring:
  validate_every_n_batches: 1000 # Run validation every N batches (was 100 - validation is slow!)
  validation_samples: 50 # Number of validation samples to evaluate (was 100)
  inference_every_n_batches: 500 # Generate sample every N batches (was 50 - inference is slow!)
  inference_prompt: "Once upon a time" # Prompt for sample generation
  inference_max_tokens: 50 # Max tokens to generate
  inference_temperature: 1.0 # Sampling temperature
  graph_window_size: 1000 # Number of points to show in graphs
